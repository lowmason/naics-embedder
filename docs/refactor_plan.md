# Refactor plan

## Deep check findings
- **Command-line surface area lacks documentation**: CLI modules expose many commands, but docstrings and MkDocs coverage are incomplete, making the workflow hard to navigate programmatically.
- **Training command responsibilities are tightly coupled**: `train` mixes hardware inspection, configuration parsing, checkpoint selection, trainer wiring, and optional embedding generation in a single function, which reduces testability and makes error handling brittle.
- **Sequential training pathway is deprecated but still prominent**: The legacy sequential trainer remains alongside the primary flow without clear separation, increasing maintenance overhead and user confusion.
- **Warning suppression is duplicated**: Multiple modules suppress the same PyTorch Lightning warnings, leading to scattered configuration that can drift or hide useful signals.
- **Tokenization and data access assumptions are implicit**: Data paths and tokenization caches are inferred from configuration without validation, which can cause runtime surprises when files are missing or incompatible.

## Recommended refactors
1. **Clarify the public CLI API**
   - Document commands via Google-style docstrings and MkDocs pages generated by `mkdocstrings`.
   - Add parameter validation for common options (paths, stages) and surface concise error messages.
2. **Decompose training orchestration**
   - Extract hardware introspection, override parsing, checkpoint resolution, and Trainer creation into dedicated helper functions or classes in `naics_embedder.utils`.
   - Introduce a small orchestration object that returns a structured result (paths, metrics) for easier testing and reuse.
3. **Isolate deprecated flows**
   - Move sequential training into a dedicated module flagged as legacy, and gate it behind an explicit `--legacy` flag or environment variable.
   - Provide migration guidance in docs pointing users to the preferred curriculum implementation.
4. **Centralize warning configuration**
   - Create a `naics_embedder.utils.warnings` module that applies shared suppression settings once, reducing duplication and making adjustments auditable.
5. **Harden data and tokenization inputs**
   - Add lightweight pre-flight checks that confirm parquet schemas and tokenization cache compatibility before training or embedding generation begins.
   - Emit actionable remediation steps when inputs are missing or stale (e.g., regenerate cache, rerun preprocessing).
6. **Improve observability and outputs**
   - Standardize logging fields (stage, experiment, device) and add summary artifacts (YAML/JSON) after training to feed downstream evaluation and MkDocs examples.

## Documentation integration
- Ensure all public functions, classes, and Typer commands include Google-style docstrings so `mkdocstrings` can render consistent API pages.
- Add a short "CLI quickstart" section to the MkDocs site that links command groups to their generated documentation and explains common workflows (data prep, training, visualization).
