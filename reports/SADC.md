# Evaluation of the SADC Sampling Strategy for NAICS Hyperbolic Embedding

## Theoretical Soundness: Hierarchy and Hyperbolic Geometry Alignment

**Preserving NAICS Hierarchical Structure:** The Structure-Aware Dynamic Curriculum (SADC) is designed to respect the semantic structure of the NAICS taxonomy. NAICS codes form a tree from broad sectors (2-digit) to fine-grained industries (6-digit), where sibling codes (e.g. 541511 vs 541512) are much more semantically similar than distant codes (e.g. 541511 vs 111110 "Soybean Farming"). In standard contrastive learning, however, a distant code like *Soybean Farming* would be treated as an equally valid negative to the anchor 541511 as the close sibling 541512. The model would quickly separate obviously unrelated classes (driving those easy negatives' loss contribution to nearly zero) and stop getting gradients to learn finer distinctions. SADC addresses this by *dynamically biasing negative sampling toward semantically relevant negatives*. Early in training, it leverages the known tree distances to select *"cousins"* (codes sharing a grandparent in the taxonomy) as negatives and **masks siblings entirely**. This prevents the model from prematurely pushing nearly identical siblings apart, which would *"shatter" sibling clusters and create a void where the parent concept should reside*. Theoretically, this is sound: it acknowledges the **gradient-semantic trade-off** -- easy, distant negatives yield no learning signal, while extremely hard negatives (siblings) yield high gradient but risk being false negatives. By starting with intermediate-hard negatives (cousins), SADC ensures the model gets meaningful contrastive signals without violating the taxonomy's semantic structure.

**Hyperbolic Embedding Considerations:** The repository embeds NAICS codes in **Lorentzian hyperbolic space** to naturally model the tree's exponential growth in categories. This choice introduces unique sampling issues: hyperbolic space volume grows exponentially with radius, so most points lie near the boundary (the *"boundary concentration"* phenomenon). In early training, random negatives will often be far apart (near the boundary), making them *"easy negatives"* that produce negligible loss. SADC's curriculum is theoretically well-aligned with this geometry. In **Phase 1**, it does not even rely on learned distances (since the hyperbolic space is not yet well-formed); instead it uses the *explicit tree distance* as a proxy for semantic distance. This builds a coarse embedding "skeleton" of the hierarchy before hyperbolic distances are reliable. By **Phase 2**, once the model has learned a meaningful hyperbolic metric, SADC shifts to mining negatives based on **Lorentzian distance** \$d\_{\\mathcal{L}}\$ in the embedding space. This leverages the geometry: as the model's representations mature, hyperbolic distance becomes a good indicator of semantic similarity, so choosing the closest points in hyperbolic space as negatives forces the model to refine subtle distinctions. The curriculum thus *bridges the gap between discrete taxonomy and continuous geometry*, using each at the appropriate time. In theory, this phased approach should produce an embedding space that is both **discriminative** (pushes apart different classes) and **hierarchy-faithful** (retains the tree's structure in distances).

Additionally, the curriculum incorporates a **norm-adaptive margin** for the triplet contrastive loss (decay margin for points with larger hyperbolic norm) to account for hyperbolic density variation. This is theoretically sound: points deeper in the hierarchy (farther from the origin) live in a denser region of space, so a smaller margin prevents over-penalizing negatives near those deep anchors. Overall, SADC's design is grounded in the theory of both the NAICS hierarchical semantics and hyperbolic geometry, addressing known pitfalls like fast saturation of loss with easy negatives and the risk of breaking local clusters.

## Phase-wise Strategy and Alignment with Model Architecture

**Phase 1 -- Structural Initialization (0--30% of training):** In the initial phase, SADC prioritizes structural cues from the NAICS tree. The negative sampling probability is weighted inversely by taxonomy distance (e.g. \$P(n\|a)\\propto 1/d\_{\\text{tree}}(a,n)\^{\\alpha}\$ with \$\\alpha\\approx1.5\$) and *siblings (distance=2) are excluded* from negatives. This means the model mostly sees "cousin" negatives -- codes that share some ancestry but are not siblings -- forcing it to learn to differentiate fine-grained categories within the same broader context. For example, an anchor in *Custom Computer Programming* might be contrasted with a cousin like *Data Processing* (common sector/subsector, different industry group) rather than something far afield like farming. This phase **"builds the skeleton"** of the embedding space: the model learns broad separation of clusters according to the taxonomy, while keeping truly close classes (siblings) together. The Mixture-of-Experts (MoE) text fusion model in the repository benefits from this as well -- by focusing on mid-level distinctions, it gives all experts useful signal. In particular, any coarse-grained experts (e.g. one specializing in top-level sector discrimination) get training signal at this stage because negatives are chosen across different sectors and industry groups, not only trivially unrelated pairs. This alignment ensures the MoE doesn't collapse early on: each expert sees a variety of moderately hard contrasts relevant to its specialization.

**Phase 2 -- Geometric Refinement (30--70%):** In the second phase, SADC gradually shifts emphasis from the symbolic tree structure to the **learned hyperbolic geometry** of the embedding. The strategy here is **"annealed" hard negative mining in Lorentz space**. Practically, the training code samples a candidate pool of negatives (some random, some still structure-weighted) and then selects the top-\$k\$ hardest negatives for each anchor based on smallest Lorentzian distance in the current embedding. By doing this, the model is now challenged with truly hard negatives *as measured by its own understanding* -- often these will be semantically very similar codes that the model is at risk of confusing. This phase directly aligns with the **hyperbolic contrastive learning architecture**: it uses the model's distance function to define "hardness," fine-tuning the decision boundaries that were roughly set in Phase 1. Importantly, Phase 2 also introduces **MoE Router-Guided Sampling**, a technique specifically tailored to the Mixture-of-Experts fusion model. The gating network (router) of the MoE outputs a probability distribution over experts for each input. SADC leverages this by mining negatives that *"confuse" the router -- i.e. cases where the anchor and a candidate negative activate the same experts with similar probabilities*. By prioritizing negatives with high router confusion (measured via a low KL-divergence or high cosine similarity between gate distributions), the training forces the MoE to sharpen its expert specializations. In essence, if two different NAICS codes appear similar enough that the MoE router thinks they require the same expert, the model will receive a contrastive signal to differentiate them. This addresses the "expert collapse" problem where one expert might otherwise handle many inputs: SADC ensures that each expert is challenged by borderline cases. The repository's implementation confirms that in Phase 2, they mix both types of hard negatives -- geometric hard negatives and router-confusing negatives -- roughly 50/50 to get the benefits of both. This combined approach aligns perfectly with the **MoE architecture**: it not only refines the embedding in the hyperbolic space but also optimizes the gating mechanism so that different experts truly specialize in different semantic regions of the taxonomy. By the end of Phase 2, we expect the model's continuous space to mirror the NAICS hierarchy's structure closely, and the MoE to have allocated experts to different aspects of the data (e.g. perhaps one expert focuses on distinguishing very fine-grained tech industry codes, while another handles coarse sector-level differences).

**Phase 3 -- False Negative Mitigation (70--100%):** In the final phase, SADC focuses on cleaning up remaining **semantic ambiguities** in the learned space. Even with careful negative sampling, some distinct NAICS codes are so semantically similar that the model may rightfully cluster them closely (for example, two codes that differ only in a minor technical definition). In vanilla contrastive learning, such cases can become **false negatives** -- the model would erroneously treat them as negatives and be forced to push them apart, harming embedding quality. SADC counters this by performing **clustering-based False Negative Elimination (FNE)**. Periodically (by default every few epochs in the last phase), the training procedure *freezes the encoder and runs a Hyperbolic K-Means clustering* on all embeddings. Each code gets assigned a pseudo-label corresponding to a cluster. Thereafter, when drawing negatives, any candidate that falls in the same cluster as the anchor is presumed semantically similar and **removed from the negative set (ignored in the loss)**. This is an *elimination* strategy: the model simply does not push those pairs apart (as opposed to reclassifying them as positives, which was deemed too risky given potential noise in clustering). This phase aligns with the overarching goal of preserving hierarchy and semantic truth in the embedding. By training's end, the model stops "fighting the data" -- if two codes are essentially synonyms or extremely close concepts, the curriculum allows them to stay close in the embedding. For the NAICS hierarchy, this is very useful. NAICS codes are sometimes split for administrative reasons while being very similar in meaning; Phase 3 lets such items remain neighbors rather than forcing artificial separation. Implementationally, this phase does introduce overhead (clustering in hyperbolic space with, say, 500 clusters and updating every 5 epochs), but it runs infrequently and yields a more coherent final representation. The **hyperbolic contrastive learning architecture** readily supports this -- the loss function in the repository accepts a mask of false negatives so that those pairs do not contribute repulsive force. By aligning training with the model's own emergent clusters, Phase 3 improves convergence and fine-tunes the embedding to reflect "semantic reality" beyond the raw labels.

In summary, each SADC phase is crafted to complement both the **NAICS hierarchy** and the model's **Mixture-of-Experts hyperbolic embedding architecture** at that stage of training. Early phases inject human knowledge of structure to guide the MoE and embedder when they are most vulnerable to mistakes, while later phases trust the model's learned geometry and specialize the experts. This phased curriculum aligns with known best practices in hierarchical contrastive learning -- essentially implementing a form of curriculum learning that goes from **"easy" (taxonomy-guided) to "hard" (model-guided) negatives**, without ever losing sight of the hierarchy's constraints.

## Computational Efficiency and Practical Implications

Implementing SADC does add computational complexity to the training pipeline. A key question is whether these overheads are justified by performance gains in practice. We consider each phase's cost and benefits:

- **Global Negative Sampling Overhead:** To achieve Phase 1 and Phase 2's sampling objectives, the repository had to go beyond naive in-batch sampling. For instance, ensuring a good selection of *"cousin"* negatives and performing hard-negative mining often requires a **large candidate pool**. The project addressed this by aggregating data across GPUs: *"Local micro-batches (e.g., size 32) are too small to contain meaningful \'Cousin\' negatives. We must sample negatives from the global batch across all GPUs."*. Practically, this means using a distributed all-gather to collect embeddings from all workers each step, then computing similarity or gate confusion over a much larger set. This **global batch sampling** ensures the model can find truly hard negatives (which might have been on another GPU or outside a small batch), improving training efficacy. The cost is additional communication and an \$O(N\_{\\text{global}}\^2)\$ operation to evaluate negatives. In a multi-GPU setting with a global batch size \$N\_{\\text{global}}\$, computing a full similarity matrix or even doing top-\$k\$ search can be memory-intensive. Indeed, Issue #19 in the repo logs the need to *"verify VRAM usage when computing the \$N\_{global}\\times N\_{global}\$ similarity matrix."*. The implementation mitigates this by chunking computations (as seen in the code looping over anchors for router confusion scoring) and by limiting \$k\$ (only the top few negatives are selected rather than using all pairs). The overhead of all-gather and top-\$k\$ mining is **significant but manageable** for the scale of NAICS: the number of classes (on the order of 1000) is not huge, and global batch sizes are in the low hundreds, so these operations are feasible. Crucially, the benefits likely outweigh the cost -- without global hard-negative mining, the model might never see some of the most informative negative comparisons, resulting in poorer convergence or lower final accuracy. In other words, SADC's sampling strategy *spends extra computation to extract more learning signal from the data*, which is usually a good trade-off in deep learning when done judiciously.

- **Mixture-of-Experts Router Computations:** The router-guided negative selection in Phase 2 introduces another layer of computation. For each anchor, not only must we compute embedding distances, but we also compute the **gating probability distributions** for many candidate negatives and compare them (via KL-divergence or cosine) to the anchor's distribution. In the repository, this is implemented by recording the router's softmax outputs for all samples and then scoring negatives by distribution similarity. This can be computationally heavy -- essentially an extra forward pass through the gating network for negatives, though in practice the gating probabilities can be obtained as a byproduct of the forward encoding of negatives (the MoE gating is part of the encoder output). The code dynamically mixes half router-confusing negatives and half distance-based negatives, which means this router-based scoring doesn't need to be applied to *all* negatives, just enough to pick the top confusing ones. There is also a fallback to using local batch gating or embedding-based mining if needed. While this adds to the training time, it is a relatively small overhead compared to the transformer encoding of text -- computing a KL divergence between a few probability vectors is trivial next to running four LoRA-adapted transformer encoders. Thus, the *relative* cost of router-guided sampling is low, and it directly addresses MoE performance (preventing one expert from dominating), likely improving long-term throughput by avoiding mode collapse. In practice, the confusion-based negatives help keep all experts engaged, which can lead to better utilization of model capacity and potentially faster convergence (since each expert learns something unique). The repository logs show monitoring of router confusion scores, indicating that this mechanism is actively shaping training dynamics. The slight increase in per-step computation is a reasonable price for maintaining robust expert diversity.

- **Hyperbolic Clustering Overhead:** Phase 3's false-negative mitigation via hyperbolic K-means clustering is perhaps the most expensive component in theory. Clustering all embeddings into (e.g.) 500 clusters with 100 iterations every 5 epochs is an \$O(C \\times N \\times D)\$ operation (for \$C\$ clusters, \$N\$ points, \$D\$ dimensions) that could be heavy if \$N\$ (the number of items to embed) were very large. In the NAICS use case, however, \$N\$ is on the order of the number of codes or training samples -- likely a few thousand at most -- which keeps this feasible. The **Hyperbolic K-means** is more complex than Euclidean k-means (because distance computations and centroid updates occur in Lorentz space), but given modern hardware and the modest size, it should run in seconds per update. The curriculum only activates clustering in the final 30% of training and updates periodically (default every 5 epochs), which means the amortized cost per epoch is not high. The repository's code ensures clustering runs with the encoder *frozen* (no gradient), so it can be done out-of-graph; it only needs to produce the pseudo-label mapping for negatives. Thus, the clustering adds some overhead but not to the main backpropagation loop except when incorporating the mask for false negatives (a simple tensor compare operation). The benefit of this clustering is potentially **significant for final model quality**: it directly addresses cases that are otherwise intractable for the loss function (high semantic overlap between different labels). By removing those conflicts, the model can use its capacity to focus on meaningful distinctions. In real-world terms, this could improve metrics like clustering purity or nearest-neighbor retrieval accuracy for the embedding. There is a risk that if clustering is inaccurate (e.g., chooses too broad clusters), it might remove genuinely informative negatives. However, SADC's design choices (waiting until late in training and using elimination rather than pulling negatives together) minimize harm from any clustering noise. The computational cost here is justified if it yields even a few percent improvement in representation quality or convergence stability -- especially since the final phase of training often sees diminishing returns, this step can squeeze out remaining improvements.

- **Overall Training Throughput:** Combining all these aspects, it's clear SADC training is more complex than a standard contrastive learning loop. There are additional synchronization points (for global negatives), extra data structures (for tree distances, gating outputs, cluster labels), and phase-specific behaviors to manage. The repository implements a **CurriculumScheduler** to handle phase transitions and flags, which adds code complexity but ensures the overhead is only incurred when needed for that phase. From a practical perspective, training with SADC will run slower per epoch than a naive approach. However, it may reach a better model in fewer epochs or avoid getting stuck in bad minima. The **optimality** of this strategy thus depends on the balance of *time vs. performance*. Given that NAICS classification is a difficult task (hierarchical, with potentially many subtle distinctions), the extra computations are likely worthwhile. Each component of SADC is targeted at a known challenge (insufficient gradient from easy negatives, expert under-utilization, false negative pairs). Without these fixes, one might have to compensate by tuning other hyperparameters or accepting a lower-performing model. For example, the alternative to dynamic hard-negative mining might be to drastically increase training data or augmentations to get enough hard examples by chance. SADC instead *spends compute to create effectively "harder data"* from a fixed dataset, which is usually a smart trade in deep learning when data is limited.

In conclusion, while SADC does introduce **non-trivial overhead**, the strategy is carefully designed so that each cost has a corresponding benefit to model learning. The repository developers explicitly flagged and solved these efficiency issues (global gathering, memory management, etc.), indicating that the practical impact was manageable. Unless operating under severe time or compute constraints, the improvements in training dynamics and final accuracy from SADC should outweigh the slower per-step speed. For the NAICS embedder, which is a one-time training for a highly specialized model, **maximizing representation quality is likely more important than shaving off training time** -- and SADC serves that goal well.

## Strengths of the SADC Strategy

- **Respects Hierarchical Semantics:** SADC explicitly incorporates the NAICS hierarchy into training. This leads to embeddings that preserve parent-child and sibling relationships more faithfully than a one-size-fits-all sampling. The Phase 1 tree-distance weighting *"guides the embedding space to reflect the tree structure"* from the start, mitigating the risk that the model ignores subtle intra-sector differences. As a result, the learned embeddings are **hierarchically organized**, which is crucial for a taxonomy like NAICS.

- **Prevents Gradient Starvation:** By dynamically adjusting negative difficulty, SADC avoids the model falling into trivial solutions. Early phase avoidance of ultra-easy negatives keeps the loss term informative (preventing it from "extinguishing the gradient signal" too soon), while later mining of hard negatives ensures the model continues to receive challenging training signals rather than plateauing. This curriculum **maximizes time in the useful learning regime**, as evidenced by the strategy to *"maximize the density of negatives in the \'Hard\' regime while ensuring they are true negatives"*.

- **Synergy with Hyperbolic Space:** The strategy brilliantly leverages the strengths of hyperbolic geometry. Hyperbolic space is known to embed trees with low distortion; SADC capitalizes on this by using the Lorentzian distance as a guide once it becomes meaningful. The introduction of a **norm-adaptive margin** and focus on near-boundary points (where most hard negatives lie) align with hyperbolic properties. This leads to a representation that is both discriminative and geometrically well-formed for the hierarchy (e.g., preserving relative distances across levels).

- **Enhanced Mixture-of-Experts Performance:** SADC is tailored to the repository's MoE fusion model. The router-guided sampling is a novel addition that directly addresses MoE training issues (it was noted as a *"novel contribution for your specific MoE architecture"*). By forcing the gating network to confront confusing cases, the curriculum encourages **diverse expert specialization**, preventing one expert from dominating. This likely improves the model's ability to handle heterogeneous input types (one of the reasons MoE was chosen) and avoids wasted capacity. In effect, SADC not only trains the embedding space but also the gating function to be more effective.

- **False Negative Reduction:** The clustering-based elimination of false negatives is a strong advantage for a dense label space like NAICS. It acknowledges that the taxonomy, while hierarchical, isn't a perfect reflection of semantic similarity (some codes are essentially equivalent in meaning). Removing those false negatives late in training helps the model consolidate clusters that belong together, improving final embedding quality (higher clustering purity, better retrieval of similar items, etc.). It provides a safety net that pure contrastive loss lacks, likely resulting in higher correlation between embedding distances and true semantic distances by the end.

- **Comprehensive Multi-Phase Solution:** Overall, SADC's phased approach is a **holistic solution** to multiple issues. Each phase targets a different problem (hierarchical initialization, metric learning refinement, semantic cleanup) without any of these being left unaddressed. This comprehensive coverage means the final model is robust on many fronts: it won't have trivial collapse in early training, it won't have under-trained experts, and it won't have obvious false-negative distortions. Few sampling strategies cover this range of challenges in one framework.

## Limitations and Challenges

- **Implementation Complexity:** SADC's multi-phase curriculum is complex to implement and maintain. It introduces many moving parts: custom sampling logic, phase scheduling, dynamic flags, a special clustering step, etc. The repository code had to handle distributed data gathering, on-the-fly probability computations, and more, which increases the potential for bugs or mis-tuned parameters. For example, ensuring gradients flow correctly through the all-gather or that clustering happens at the right time requires careful engineering. This complexity means higher development and debugging effort compared to a standard contrastive training loop.

- **Hyperparameter Sensitivity:** The dynamic nature of SADC means there are additional hyperparameters that can affect performance: the phase transition points (e.g. 30% and 70% of training), the weighting exponent \$\\alpha\$ for tree distance, the number of hard negatives \$k\$, the mix ratio for router-based negatives, cluster count, etc. The optimal settings might differ by dataset or model size. If these are not chosen well, the curriculum could either move too slowly (wasting time on easy negatives) or too quickly (exposing the model to hard negatives before it's ready). In other words, **curriculum learning itself must be tuned**. The chosen 0--30--70% schedule and other defaults seem reasonable for NAICS, but one could imagine edge cases where siblings actually have distinguishing features that the model could handle earlier, or conversely scenarios where even 30% of training is not enough warm-up.

- **Computational Overhead:** As discussed, SADC adds overhead in terms of memory and compute. While we argued the costs are justified, it's still true that training with SADC will be slower and use more memory than a simpler scheme. In resource-constrained environments or for extremely large datasets, this could be a limitation. For instance, clustering all embeddings might become infeasible if \$N\$ were in the millions. Likewise, global negative mining might become too communication-heavy beyond a certain number of GPUs or batch size. Thus, SADC in its current form is optimal for the given scale, but may not trivially scale to *arbitrarily large* problems without additional engineering (e.g., using approximate nearest neighbors for negative mining instead of brute force).

- **Late Correction of False Negatives:** The false-negative mitigation happens only in the final phase (after \~70% of training). This is by design (to wait for a stable embedding), but it means for most of training, the model might still treat some semantic twins as negatives. If two codes are virtually identical in meaning, the model will spend many epochs pushing them apart before the clustering step intervenes. This could slow convergence or even cause representational damage that is hard to undo completely. In theory, one might want a mechanism to flag obvious false negatives earlier. SADC's conservative approach avoids false positives, but the trade-off is that some negative pairs that **should** be treated as positives are only addressed in the last 30% of training. If the clustering is not perfectly timed, there's a risk that the model has already "decided" to embed certain siblings far apart. (That said, the norm-adaptive margin and the Phase 1 sibling masking help reduce the damage done early on, so this is a minor concern.)

- **Potential Overfitting to Taxonomy:** By relying heavily on the NAICS provided structure (especially in Phase 1), there is a chance the model overfits to the hierarchical relations as given, potentially overlooking alternative groupings present in the textual data. NAICS is a human-defined ontology; the model might be constrained to that view and might not discover any cross-hierarchy similarities early on. The curriculum does allow discovering new semantic structure in Phase 2 (when it switches to learned distances), but the strong prior in Phase 1 could bias the representation. If the NAICS taxonomy has any imperfections or arbitrary distinctions, the model will faithfully encode them. In applications where the hierarchy might be updated or where one wants the model to sometimes challenge the ontology, a less rigid sampling at start might be preferable. Essentially, SADC optimizes for *faithfulness to the given hierarchy*, which is a strength for consistency, but could be a limitation if the underlying taxonomy is incomplete or one wishes the model to generalize beyond it.

- **Not Universally Applicable:** The SADC strategy is highly specialized for this kind of hierarchical, hyperbolic embedding scenario. For a flat classification or a non-hierarchical embedding task, much of this would be overkill or inapplicable. Even within hierarchical tasks, if one doesn't use a hyperbolic space or an MoE architecture, certain phases (like router-guided sampling or norm-adaptive margins) wouldn't apply. Thus, the *optimality* of SADC is somewhat contextual -- it's optimal for the NAICS embedder's design and objectives, but it's not a one-size-fits-all sampling strategy. This is less a criticism of SADC itself and more a note that its complexity is justified by a very specific problem setting.

## Alternatives and Potential Enhancements

While SADC appears to be a well-crafted solution for the current repository, it's worth considering alternatives or tweaks, either to simplify the approach or to address any gaps:

- **Static or Simpler Sampling Strategies:** A baseline alternative would be a **static structure-aware negative sampling** (as opposed to dynamic curriculum). For example, one could use a fixed sampling distribution that always biases toward near negatives (cousins/siblings) with some probability and includes distant ones occasionally. This would resemble the **Structure-Aware Negative Sampling (SANS)** approach from prior work (reference 15 in the PDF) without the phased curriculum. The advantage of a static strategy is simplicity -- no phases to tune, and easier implementation. However, static strategies struggle to balance the trade-off throughout training: if you include siblings from the start, you risk false negatives early; if you don't include them at all, the model might never learn to tell very similar classes apart. SADC essentially was proposed because static weighting proved *"ill-suited for the NAICS hierarchy, especially when embedded in a hyperbolic manifold"*. Thus, while a static negative sampling could reduce overhead, it would likely yield inferior performance or require careful manual weighting to approximate what SADC achieves adaptively.

- **Two-Phase Curriculum or Continuous Annealing:** If one wanted to simplify, a possibility is to merge SADC's three phases into two broader stages or even a continuous schedule. For example, Phase 3's false negative handling could be combined with Phase 2 by slowly introducing clustering-based filtering once the model's embedding quality (e.g. measured by some metric) crosses a threshold, rather than a hard epoch cutoff. Likewise, Phase 1 and 2 could be blended: gradually decrease the influence of tree-distance sampling and increase the influence of learned-distance mining over time (a form of *continuous annealing* of \$\\alpha\$ or the sampling weights). This would remove the need to determine exact epoch cutoffs and might adapt more smoothly. The risk, however, is that a continuous approach might activate certain behaviors too early or too late if not carefully calibrated. The discrete phase approach is easier to reason about (and implement with flags), whereas continuous schedules introduce their own hyperparameters (rates, temperature schedules, etc.). Nonetheless, for some datasets a more gradual transition might avoid sudden changes in training dynamics. This could be an enhancement if one observes instability when phases switch; using a smoother interpolation between strategies could make the curriculum more robust.

- **Enhanced False Negative Handling:** The current strategy uses clustering to eliminate false negatives late in training. An alternative enhancement could be an **attraction-based** false negative handling or semi-supervised approach: once clusters of likely-similar codes are identified, one might add a small attraction force or treat them as positives in an auxiliary loss. The PDF notes that an *"attraction" strategy (relabeling false negatives as positives) is less tolerant to noise* and thus recommends elimination. However, if the clustering is highly accurate, attraction could further tighten those clusters. Another approach could be to incorporate domain knowledge: for example, use the NAICS "excluded" field relations (which Phase 1 already leverages as high-priority negatives) to also inform false negatives. If two codes are never listed as exclusions of each other and often co-occur in text context, one might proactively mark them as potential false negatives. These ideas add complexity but could catch semantic equivalences earlier. In practice, the elimination method in SADC is a conservative choice that likely covers most needs; improvements here would require careful precision to avoid introducing false positives.

- **Multi-Level Contrastive Learning:** The strategy discussed in the PDF alongside SADC is the *"Use All The Labels"* approach (HiMulCon), where each sample is used to generate contrastive pairs at multiple levels of the hierarchy (sector-level, subsector-level, etc.). This is actually implemented in the repository (Issue #18) as an augmentation to the training data (expanding the batch with multiple positives at different levels) and is complementary to SADC. If not already fully utilized, enabling multi-level supervised contrastive loss is a highly promising enhancement. It would ensure that the model learns not just leaf-level distinctions but also directly optimizes the separation of higher-level categories. For the MoE, this is very valuable: it can dedicate some experts to coarse distinctions (with gradients coming from high-level contrastive losses) and others to fine distinctions. In essence, this provides another **curriculum in label granularity** running in parallel to SADC's curriculum in negative hardness. The combination of multi-level supervision with SADC yields a powerful signal: e.g., an anchor's sector "54" (Professional Services) will be pulled closer to another code in sector 54 and pushed away from codes in sector 11 (Agriculture) at the same time as, on another level, the anchor's specific code is being contrasted with its cousins. The PDF's recommendations explicitly mention *"Don't just contrast leaf codes\... contrast the entire path (Sector, Subsector, Industry) to maximize gradient flow."*. This can be seen as an enhancement to SADC (or a part of the overall optimal strategy) that the repository has embraced. It might increase training time (since the batch effectively multiplies), but it greatly enriches the training signal and aligns with the MoE design.

- **Ablating Components (Cost-Benefit):** To assess optimality, one could consider if any phase of SADC could be dropped with minimal loss. For instance, is Phase 3 absolutely needed, or would a well-trained Phase 1+2 suffice for good embeddings? It's possible to imagine a scenario where if the taxonomy is perfectly semantically consistent, false negatives might be rare. Removing clustering would save some complexity. However, given NAICS's real-world complexity, Phase 3 provides a nice safeguard. Conversely, one might consider if Phase 1 could be skipped in favor of starting directly with learned distance mining. This would simplify things but likely would fail, as the model would get overwhelmed by hard negatives or waste time on easy ones initially. The phased results seem to address genuine needs at each stage. Thus, there are not many "low-hanging fruit" components to remove without impacting performance. The more fruitful angle is to look for **automations** -- for example, automatically detecting when to transition phases based on metrics (like when the model's hierarchy preservation score plateaus, switch to Phase 2, etc.) rather than pre-setting epoch percentages. This could make the curriculum more adaptive to training progress. It's an enhancement that could be explored to make SADC *optimal in a wider range of scenarios* without manual phase scheduling.

- **Scaling Strategies:** If the repository were to be applied to a larger taxonomy or dataset, some alternatives to the brute-force methods used might be needed. For example, instead of all-gathering full batches for negatives, one could maintain a **memory bank** of embeddings to draw hard negatives from, or use approximate nearest neighbor search in embedding space to find hard negatives more efficiently than computing all pairs each time. These are more engineering-focused alternatives to keep the spirit of SADC (mining hard negatives) while controlling computational cost. Likewise, for clustering a very large dataset, one could use mini-batch clustering or hierarchical clustering to approximate the same false-negative detection. These don't change SADC's theoretical approach but are practical tweaks to ensure it remains optimal when scaling up.

In summary, SADC as implemented seems quite optimal for the NAICS embedder's needs -- it thoughtfully combines ideas from static structure-aware sampling, curriculum learning, hard negative mining, and expert gating techniques. Alternatives like a simpler static scheme or partial curriculum would likely underperform in this specific context, though they could be considered if one needed a lighter-weight solution. Enhancements like multi-level contrast (already recommended and implemented) and possibly more adaptive phase transitions could further improve the robustness of the approach. However, the **strength of SADC is that it is already a synthesized "best-of-all-worlds" strategy**, incorporating numerous insights from literature (hierarchical contrastive learning, false-negative mitigation, etc.) into one framework. Any tweaks should be made carefully to not disturb this balance.

## Conclusion

Considering the above points, the Structure-Aware Dynamic Curriculum appears to be a **well-founded and effective sampling strategy for the** `lowmason/naics-embedder` **project**. Theoretically, it aligns perfectly with the challenges posed by the NAICS hierarchy and hyperbolic embedding model, addressing the shortcomings of naive sampling in this context. Empirically, although it increases training complexity, each component of SADC is aimed at a critical issue that would likely hurt performance if left unchecked. The phased curriculum harmonizes with the repository's Mixture-of-Experts architecture and hyperbolic contrastive loss, leading to a training process that is complex but *comprehensive*. In terms of optimality, one might say SADC is **optimal in the sense of yielding a high-quality model**, even if it's not minimal in compute or simplicity. Its strengths in preserving hierarchical relationships, maintaining gradient signal, and enhancing expert specialization far outweigh the added costs, especially for a taxonomy as nuanced as NAICS. There may be room for minor improvements or parameter tuning, but overall SADC can be judged as a suitable and likely near-optimal strategy for this specific project.

In conclusion, **SADC is a strategically optimal choice for** `naics-embedder` **given its goals** -- it integrates structure and learning dynamics in a way that few other sampling strategies could, and it sets a strong foundation for the model to learn a *"true, hierarchically consistent semantic manifold"* for industries. The careful balance of theory and practice in SADC exemplifies how a deep understanding of both the data (NAICS taxonomy) and the model (Lorentz hyperbolic MoE) can drive the design of a training curriculum that leads to better outcomes than any static heuristic.
