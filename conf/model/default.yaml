# Model Configuration: conf/model/default.yaml
name: "NAICSEmbedder"
base_transformer: "sentence-transformers/all-mpnet-base-v2"

# --- Architecture ---
projection_dim: 512
final_embedding_dim: 256 # Dimension of the final hyperbolic embedding

# --- LoRA ---
lora:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules: ["q_proj", "v_proj"]

# --- Mixture of Experts (MoE) ---
moe:
  num_experts: 8
  top_k: 2
  aux_loss_coeff: 1e-2

# --- Hyperbolic Space ---
hyperbolic_curvature: 1.0

# --- Optimizer & Scheduler ---
learning_rate: 1e-4
lr_warmup_steps: 500
weight_decay: 0.01
