{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"NAICS Hyperbolic Embedding System \u00b6 This documentation describes the complete hyperbolic representation learning system for the NAICS taxonomy. The system integrates: Multi-channel transformer-based text encoding Mixture-of-Experts fusion Lorentz-model hyperbolic contrastive learning False-negative mitigation via curriculum clustering Hyperbolic Graph Convolutional refinement (HGCN) Use the navigation menu to explore system architecture, training procedures, and API references for each module.","title":"Home"},{"location":"#naics-hyperbolic-embedding-system","text":"This documentation describes the complete hyperbolic representation learning system for the NAICS taxonomy. The system integrates: Multi-channel transformer-based text encoding Mixture-of-Experts fusion Lorentz-model hyperbolic contrastive learning False-negative mitigation via curriculum clustering Hyperbolic Graph Convolutional refinement (HGCN) Use the navigation menu to explore system architecture, training procedures, and API references for each module.","title":"NAICS Hyperbolic Embedding System"},{"location":"architecture/","text":"System Architecture Documentation \u00b6","title":"System Overview"},{"location":"architecture/#system-architecture-documentation","text":"","title":"System Architecture Documentation"},{"location":"embeddings/","text":"NAICS Hyperbolic Embedding System \u2014 HGCN Refinement Guide \u00b6 Overview \u00b6 This document explains the final stage of the NAICS hyperbolic embedding pipeline: refinement using a Hyperbolic Graph Convolutional Network (HGCN). 1. Purpose of HGCN Refinement \u00b6 Integrates NAICS taxonomy directly into embedding geometry. 2. Input Requirements \u00b6 Lorentz hyperbolic embeddings NAICS parent\u2013child graph Level metadata 3. Running the Refinement \u00b6 python train_hgcn.py --config configs/hgcn.yaml 4. HGCN Layer Operation \u00b6 Each layer performs log-map, graph convolution in tangent space, activation, and exp-map. 5. Refinement Loss Functions \u00b6 Hyperbolic Triplet Loss Per-Level Radial Regularization 6. Learnable Curvature \u00b6 Curvature parameter is optimized jointly. 7. Output of HGCN Refinement \u00b6 Refined Lorentz-model hyperbolic embeddings aligned with taxonomy structure.","title":"HGCN Refinement"},{"location":"embeddings/#naics-hyperbolic-embedding-system-hgcn-refinement-guide","text":"","title":"NAICS Hyperbolic Embedding System \u2014 HGCN Refinement Guide"},{"location":"embeddings/#overview","text":"This document explains the final stage of the NAICS hyperbolic embedding pipeline: refinement using a Hyperbolic Graph Convolutional Network (HGCN).","title":"Overview"},{"location":"embeddings/#1-purpose-of-hgcn-refinement","text":"Integrates NAICS taxonomy directly into embedding geometry.","title":"1. Purpose of HGCN Refinement"},{"location":"embeddings/#2-input-requirements","text":"Lorentz hyperbolic embeddings NAICS parent\u2013child graph Level metadata","title":"2. Input Requirements"},{"location":"embeddings/#3-running-the-refinement","text":"python train_hgcn.py --config configs/hgcn.yaml","title":"3. Running the Refinement"},{"location":"embeddings/#4-hgcn-layer-operation","text":"Each layer performs log-map, graph convolution in tangent space, activation, and exp-map.","title":"4. HGCN Layer Operation"},{"location":"embeddings/#5-refinement-loss-functions","text":"Hyperbolic Triplet Loss Per-Level Radial Regularization","title":"5. Refinement Loss Functions"},{"location":"embeddings/#6-learnable-curvature","text":"Curvature parameter is optimized jointly.","title":"6. Learnable Curvature"},{"location":"embeddings/#7-output-of-hgcn-refinement","text":"Refined Lorentz-model hyperbolic embeddings aligned with taxonomy structure.","title":"7. Output of HGCN Refinement"},{"location":"training/","text":"NAICS Hyperbolic Embedding System \u2014 Contrastive Training Guide \u00b6 Overview \u00b6 This document describes how to train the contrastive representation learning model that forms the first three stages of the NAICS Hyperbolic Embedding System. The objective of this stage is to produce Lorentz-model hyperbolic embeddings that capture both semantic and hierarchical relationships among NAICS codes. 1. Training Pipeline \u00b6 The contrastive training procedure includes: Multi-channel transformer encoding for each NAICS text field Mixture-of-Experts (MoE) fusion with Top-2 gating Hyperbolic projection using the Lorentz exponential map Hyperbolic InfoNCE loss using Lorentzian distance False-negative mitigation using clustering-based pseudo-labels 2. Running the Training Script \u00b6 python train.py --config configs/contrastive.yaml 3. Multi-Channel Encoding \u00b6 Each NAICS code provides four text channels: Title Description Examples Excluded codes 4. Mixture-of-Experts Fusion \u00b6 The MoE module performs conditional fusion of the text channels. 5. Hyperbolic Projection (Lorentz Model) \u00b6 The fused Euclidean vector is projected into the Lorentz model using the exponential map. 6. Hyperbolic Contrastive Loss (InfoNCE) \u00b6 Contrastive learning uses Lorentzian geodesic distance. 7. False-Negative Mitigation \u00b6 Late-training clustering identifies semantically similar negatives to exclude. 8. Output of Training \u00b6 Produces Lorentz hyperbolic embeddings with global semantic structure.","title":"Contrastive Model"},{"location":"training/#naics-hyperbolic-embedding-system-contrastive-training-guide","text":"","title":"NAICS Hyperbolic Embedding System \u2014 Contrastive Training Guide"},{"location":"training/#overview","text":"This document describes how to train the contrastive representation learning model that forms the first three stages of the NAICS Hyperbolic Embedding System. The objective of this stage is to produce Lorentz-model hyperbolic embeddings that capture both semantic and hierarchical relationships among NAICS codes.","title":"Overview"},{"location":"training/#1-training-pipeline","text":"The contrastive training procedure includes: Multi-channel transformer encoding for each NAICS text field Mixture-of-Experts (MoE) fusion with Top-2 gating Hyperbolic projection using the Lorentz exponential map Hyperbolic InfoNCE loss using Lorentzian distance False-negative mitigation using clustering-based pseudo-labels","title":"1. Training Pipeline"},{"location":"training/#2-running-the-training-script","text":"python train.py --config configs/contrastive.yaml","title":"2. Running the Training Script"},{"location":"training/#3-multi-channel-encoding","text":"Each NAICS code provides four text channels: Title Description Examples Excluded codes","title":"3. Multi-Channel Encoding"},{"location":"training/#4-mixture-of-experts-fusion","text":"The MoE module performs conditional fusion of the text channels.","title":"4. Mixture-of-Experts Fusion"},{"location":"training/#5-hyperbolic-projection-lorentz-model","text":"The fused Euclidean vector is projected into the Lorentz model using the exponential map.","title":"5. Hyperbolic Projection (Lorentz Model)"},{"location":"training/#6-hyperbolic-contrastive-loss-infonce","text":"Contrastive learning uses Lorentzian geodesic distance.","title":"6. Hyperbolic Contrastive Loss (InfoNCE)"},{"location":"training/#7-false-negative-mitigation","text":"Late-training clustering identifies semantically similar negatives to exclude.","title":"7. False-Negative Mitigation"},{"location":"training/#8-output-of-training","text":"Produces Lorentz hyperbolic embeddings with global semantic structure.","title":"8. Output of Training"},{"location":"api/encoder/","text":"Encoder API \u00b6 MultiChannelEncoder \u00b6 Bases: Module forward ( channel_inputs ) \u00b6 Forward pass through multi-channel encoder. Parameters: Name Type Description Default channel_inputs Dict [ str , Dict [ str , Tensor ]] Dict mapping channel names to tokenized inputs Each channel has 'input_ids' and 'attention_mask' required Returns: Type Description Dict [ str , Optional [ Tensor ]] Dict with 'embedding' and optional 'load_balancing_loss'","title":"Encoder"},{"location":"api/encoder/#encoder-api","text":"","title":"Encoder API"},{"location":"api/encoder/#naics_embedder.model.encoder.MultiChannelEncoder","text":"Bases: Module","title":"MultiChannelEncoder"},{"location":"api/encoder/#naics_embedder.model.encoder.MultiChannelEncoder.forward","text":"Forward pass through multi-channel encoder. Parameters: Name Type Description Default channel_inputs Dict [ str , Dict [ str , Tensor ]] Dict mapping channel names to tokenized inputs Each channel has 'input_ids' and 'attention_mask' required Returns: Type Description Dict [ str , Optional [ Tensor ]] Dict with 'embedding' and optional 'load_balancing_loss'","title":"forward"},{"location":"api/hgcn/","text":"HGCN API \u00b6 NAICSContrastiveModel \u00b6 Bases: LightningModule on_validation_epoch_end () \u00b6 Compute evaluation metrics and trigger pseudo-label update based on the curriculum schedule.","title":"HGCN"},{"location":"api/hgcn/#hgcn-api","text":"","title":"HGCN API"},{"location":"api/hgcn/#naics_embedder.model.naics_model.NAICSContrastiveModel","text":"Bases: LightningModule","title":"NAICSContrastiveModel"},{"location":"api/hgcn/#naics_embedder.model.naics_model.NAICSContrastiveModel.on_validation_epoch_end","text":"Compute evaluation metrics and trigger pseudo-label update based on the curriculum schedule.","title":"on_validation_epoch_end"},{"location":"api/hyperbolic/","text":"Hyperbolic API \u00b6 exp_map_zero ( v , c = 1.0 ) \u00b6 Exponential map from the tangent space at the origin to the hyperboloid. lorentz_distance ( u , v , c = 1.0 ) \u00b6 Calculate the distance between two points in the Lorentz model. lorentz_dot ( u , v ) \u00b6 Lorentzian inner product.","title":"Hyperbolic Ops"},{"location":"api/hyperbolic/#hyperbolic-api","text":"","title":"Hyperbolic API"},{"location":"api/hyperbolic/#naics_embedder.utils.hyperbolic.exp_map_zero","text":"Exponential map from the tangent space at the origin to the hyperboloid.","title":"exp_map_zero"},{"location":"api/hyperbolic/#naics_embedder.utils.hyperbolic.lorentz_distance","text":"Calculate the distance between two points in the Lorentz model.","title":"lorentz_distance"},{"location":"api/hyperbolic/#naics_embedder.utils.hyperbolic.lorentz_dot","text":"Lorentzian inner product.","title":"lorentz_dot"},{"location":"api/loss/","text":"Loss API \u00b6","title":"Contrastive Loss"},{"location":"api/loss/#loss-api","text":"","title":"Loss API"},{"location":"api/moe/","text":"Mixture of Experts API \u00b6 MixtureOfExperts \u00b6 Bases: Module Mixture of Experts layer with top-k gating and load balancing. Each expert is a simple feedforward network. The gating network decides which experts to use for each input, and load balancing ensures experts are utilized evenly. __init__ ( input_dim , hidden_dim = 1024 , num_experts = 4 , top_k = 2 ) \u00b6 Initialize Mixture of Experts layer. Parameters: Name Type Description Default input_dim int Input dimension (e.g., 768 * 4 for 4 channels) required hidden_dim int Hidden dimension for each expert 1024 num_experts int Number of expert networks 4 top_k int Number of experts to select for each input 2 forward ( x ) \u00b6 Forward pass through MoE layer. Parameters: Name Type Description Default x Tensor Input tensor of shape (batch_size, input_dim) required Returns: Type Description Tuple [ Tensor , Tensor , Tensor ] Tuple of: - Output tensor of shape (batch_size, input_dim) - Gating probabilities of shape (batch_size, num_experts) - Top-k indices of shape (batch_size, top_k) create_moe_layer ( input_dim , hidden_dim = 1024 , num_experts = 4 , top_k = 2 ) \u00b6 Factory function to create a MoE layer. Parameters: Name Type Description Default input_dim int Input dimension required hidden_dim int Hidden dimension for experts 1024 num_experts int Number of experts 4 top_k int Number of experts to activate 2 Returns: Type Description MixtureOfExperts MixtureOfExperts module","title":"Mixture of Experts"},{"location":"api/moe/#mixture-of-experts-api","text":"","title":"Mixture of Experts API"},{"location":"api/moe/#naics_embedder.model.moe.MixtureOfExperts","text":"Bases: Module Mixture of Experts layer with top-k gating and load balancing. Each expert is a simple feedforward network. The gating network decides which experts to use for each input, and load balancing ensures experts are utilized evenly.","title":"MixtureOfExperts"},{"location":"api/moe/#naics_embedder.model.moe.MixtureOfExperts.__init__","text":"Initialize Mixture of Experts layer. Parameters: Name Type Description Default input_dim int Input dimension (e.g., 768 * 4 for 4 channels) required hidden_dim int Hidden dimension for each expert 1024 num_experts int Number of expert networks 4 top_k int Number of experts to select for each input 2","title":"__init__"},{"location":"api/moe/#naics_embedder.model.moe.MixtureOfExperts.forward","text":"Forward pass through MoE layer. Parameters: Name Type Description Default x Tensor Input tensor of shape (batch_size, input_dim) required Returns: Type Description Tuple [ Tensor , Tensor , Tensor ] Tuple of: - Output tensor of shape (batch_size, input_dim) - Gating probabilities of shape (batch_size, num_experts) - Top-k indices of shape (batch_size, top_k)","title":"forward"},{"location":"api/moe/#naics_embedder.model.moe.create_moe_layer","text":"Factory function to create a MoE layer. Parameters: Name Type Description Default input_dim int Input dimension required hidden_dim int Hidden dimension for experts 1024 num_experts int Number of experts 4 top_k int Number of experts to activate 2 Returns: Type Description MixtureOfExperts MixtureOfExperts module","title":"create_moe_layer"}]}