{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f0e4df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------\n",
    "# Imports and settings\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "import operator\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass, fields, replace\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterator, List, Optional, Tuple\n",
    "\n",
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "import torch\n",
    "from pyarrow import dataset as ds\n",
    "\n",
    "from naics_gemini.utils.utilities import get_indices_codes\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# Configuration\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class CurriculumConfig:\n",
    "\n",
    "    codes_parquet: str = './data/naics_descriptions.parquet'\n",
    "    distance_parquet: str = './data/naics_distances.parquet'\n",
    "    relation_parquet: str = './data/naics_relations.parquet'\n",
    "    triplets_parquet: str = './data/naics_training_pairs'\n",
    "\n",
    "    anchor_level: Optional[List[int]] = None\n",
    "    positive_level: Optional[List[int]] = None\n",
    "    negative_level: Optional[List[int]] = None\n",
    "\n",
    "    anchor_distance: Optional[List[float]] = None\n",
    "    positive_distance: Optional[List[float]] = None\n",
    "    negative_distance: Optional[List[float]] = None\n",
    "    \n",
    "    n_positives: int = 2125\n",
    "    n_negatives: int = 2125\n",
    "\n",
    "    seed: int = 42\n",
    "\n",
    "    def items(self):\n",
    "        for f in fields(self):\n",
    "            if f.name != 'input_parquet':\n",
    "                v = getattr(self, f.name)\n",
    "                if v is not None:\n",
    "                    yield f.name, v\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# Utility functions\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "    \n",
    "def _get_file_list(\n",
    "    codes_parquet: str,\n",
    "    triplets_parquet: str,\n",
    "    anchor_level: Optional[List[int]] = None\n",
    ") -> List[str]:\n",
    "        \n",
    "    _, codes, codes_to_indices, _ = get_indices_codes(codes_parquet)\n",
    "\n",
    "    level_dict = defaultdict(list)\n",
    "    for code in codes:\n",
    "        level = len(code)\n",
    "        level_dict[level].append(code)\n",
    "\n",
    "    if anchor_level is not None:\n",
    "        dataset_files = []   \n",
    "        for level in anchor_level:\n",
    "            for code in level_dict[level]:\n",
    "                idx = codes_to_indices[code]\n",
    "                for pq_path in Path(f'{triplets_parquet}/anchor={idx}/').glob('*.parquet'):\n",
    "                    dataset_files.append(pq_path.as_posix())\n",
    "    else:\n",
    "        dataset_files = []\n",
    "        for pq_path in Path(f'{triplets_parquet}/').glob('**/*.parquet'):\n",
    "            dataset_files.append(pq_path.as_posix())\n",
    "    \n",
    "    return dataset_files\n",
    "\n",
    "\n",
    "def _create_dataset(\n",
    "    codes_parquet: str,\n",
    "    triplets_parquet: str,\n",
    "    anchor_level: Optional[List[int]] = None\n",
    ") -> ds.Dataset:\n",
    "    \n",
    "    dataset_files = _get_file_list(\n",
    "        codes_parquet=codes_parquet,\n",
    "        triplets_parquet=triplets_parquet,\n",
    "        anchor_level=anchor_level\n",
    "    )\n",
    "\n",
    "    print(f'Number of batches (parquet files): {len(dataset_files):,}')\n",
    "        \n",
    "    return (\n",
    "        ds\n",
    "        .dataset(\n",
    "            dataset_files, \n",
    "            format='parquet',\n",
    "            partitioning=ds.partitioning(\n",
    "                flavor='hive',\n",
    "                schema=pa.schema([\n",
    "                    pa.field('anchor', pa.uint32())\n",
    "                ])\n",
    "            )        \n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def _get_file_filters(\n",
    "    curriculum: CurriculumConfig\n",
    ") -> Optional[ds.Expression]:\n",
    "\n",
    "    exprs = []\n",
    "    for k, v in curriculum.items():\n",
    "\n",
    "        if isinstance(v, list):\n",
    "            exprs.append(\n",
    "                ds.field(k).isin(v)\n",
    "            )\n",
    "\n",
    "    if not exprs:\n",
    "        return None\n",
    "    \n",
    "    return reduce(operator.and_, exprs)\n",
    "\n",
    "\n",
    "def _get_distance_dict(\n",
    "    distance_parquet: str\n",
    ") -> Dict[Tuple[int, int], float]:\n",
    "\n",
    "    distance_iter = (\n",
    "        pl\n",
    "        .read_parquet(\n",
    "            distance_parquet\n",
    "        )\n",
    "        .select('idx_i', 'idx_j', 'distance')\n",
    "        .unique()\n",
    "        .iter_rows(named=True)\n",
    "    )\n",
    "\n",
    "    distance_dict = {}\n",
    "    for row in distance_iter:\n",
    "        key = (row['idx_i'], row['idx_j'])\n",
    "        value = row['distance']\n",
    "        distance_dict[key] = (value)\n",
    "\n",
    "    return distance_dict\n",
    "\n",
    "\n",
    "def _get_relation_dict(\n",
    "    relation_parquet: str\n",
    ") -> Dict[Tuple[int, int], Tuple[int, str]]:\n",
    "\n",
    "    relation_iter = (\n",
    "        pl\n",
    "        .read_parquet(\n",
    "            relation_parquet\n",
    "        )\n",
    "        .select('idx_i', 'idx_j', 'relation_id', 'relation')\n",
    "        .unique()\n",
    "        .iter_rows(named=True)\n",
    "    )\n",
    "\n",
    "    relation_dict = {}\n",
    "    for row in relation_iter:\n",
    "        key = (row['idx_i'], row['idx_j'])\n",
    "        value = (row['relation_id'], row['relation'])\n",
    "        relation_dict[key] = value\n",
    "\n",
    "    return relation_dict\n",
    "\n",
    "\n",
    "def _fill_incomplete(\n",
    "    incomplete_df: pl.DataFrame,\n",
    "    triplets_parquet: str,\n",
    "    curriculum: CurriculumConfig\n",
    "):\n",
    "\n",
    "    incomplete_anchors = (\n",
    "        incomplete_df\n",
    "        .get_column('anchor_idx')\n",
    "        .sort()\n",
    "        .to_list()\n",
    "    )\n",
    "\n",
    "    incomplete_files = []\n",
    "    for idx in incomplete_anchors:\n",
    "        for pq_path in Path(f'{triplets_parquet}/anchor={idx}/').glob('*.parquet'):\n",
    "            incomplete_files.append(pq_path.as_posix())\n",
    "        \n",
    "\n",
    "    curriculum_keys = [k for k, v in curriculum.items()]\n",
    "    if 'anchor_distance' in curriculum_keys:\n",
    "\n",
    "        all_distances = [\n",
    "            0.125, 0.25, 0.5, 1.0, 2.0, 2.5, 3.0, \n",
    "            3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5\n",
    "        ]\n",
    "        max_anchor_distance = max(curriculum.anchor_distance)\n",
    "\n",
    "        incomplete_distance = [d for d in all_distances if d > max_anchor_distance]\n",
    "        len_incomplete_distance = min(len(curriculum.anchor_distance), len(incomplete_distance))\n",
    "\n",
    "        incomplete_distance = sorted(list(incomplete_distance))[:len_incomplete_distance] + [7.0]\n",
    "\n",
    "    else:\n",
    "        incomplete_distance = [7.0]\n",
    "\n",
    "    incomplete_curriculum = replace(\n",
    "        curriculum,\n",
    "        anchor_distance=incomplete_distance\n",
    "    )\n",
    "\n",
    "    filter_expr = _get_file_filters(incomplete_curriculum)\n",
    "\n",
    "    incomplete_dataset = (\n",
    "        ds\n",
    "        .dataset(\n",
    "            incomplete_files, \n",
    "            format='parquet',\n",
    "            partitioning=ds.partitioning(\n",
    "                flavor='hive',\n",
    "                schema=pa.schema([\n",
    "                    pa.field('anchor', pa.uint32())\n",
    "                ])\n",
    "            )        \n",
    "        )\n",
    "        .filter(filter_expr)\n",
    "    )\n",
    "\n",
    "    incomplete_added = (\n",
    "        pl\n",
    "        .from_arrow(\n",
    "            incomplete_dataset\n",
    "            .to_table()\n",
    "        )\n",
    "        .sort('anchor_idx', 'positive_idx', 'negative_idx', 'anchor_distance')\n",
    "        .group_by('anchor_idx', 'positive_idx', maintain_order=True)\n",
    "        .agg(\n",
    "            negative_added=pl.col('negative_idx')  \n",
    "        )\n",
    "    )\n",
    "\n",
    "    completed = (\n",
    "        incomplete_df\n",
    "        .explode('positive_idx', 'negative_idx')\n",
    "        .with_columns(\n",
    "            to_add=pl.col('negative_idx')\n",
    "                    .list.len()\n",
    "                    .add(-curriculum.n_negatives)\n",
    "                    .mul(-1)\n",
    "        )\n",
    "        .join(\n",
    "            incomplete_added,\n",
    "            how='left',\n",
    "            on=['anchor_idx', 'positive_idx']\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.col('negative_added')\n",
    "            .fill_null([])\n",
    "            .list.sample(\n",
    "                pl.col('to_add'),\n",
    "                with_replacement=False,\n",
    "                shuffle=True, \n",
    "                seed=curriculum.seed\n",
    "            )\n",
    "        )\n",
    "        .select(\n",
    "            anchor_idx=pl.col('anchor_idx'),\n",
    "            positive_idx=pl.col('positive_idx'),\n",
    "            negative_idx=pl.col('negative_idx')\n",
    "                            .list.set_union(pl.col('negative_added'))\n",
    "                            .list.unique()\n",
    "        )\n",
    "        .group_by('anchor_idx')\n",
    "        .agg(\n",
    "            pl.col('positive_idx'),\n",
    "            pl.col('negative_idx')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(f'    Incomplete: {len(incomplete_files):,}, Completed: {completed.height:,}')\n",
    "\n",
    "    return completed\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# Generator function\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "def iter_file_batches(\n",
    "    dataset: Optional[ds.Dataset],\n",
    "    filter_expr: Optional[ds.Expression],\n",
    "    codes_parquet: Optional[str],\n",
    "    triplets_parquet: Optional[str],\n",
    "    curriculum: CurriculumConfig,\n",
    "    anchor_level: Optional[List[int]] = None\n",
    ") -> Iterator[Tuple[ds.FileFragment, pa.Table]]:\n",
    "    \n",
    "    if dataset is None:\n",
    "        dataset = _create_dataset(\n",
    "            codes_parquet=codes_parquet, # type: ignore\n",
    "            triplets_parquet=triplets_parquet, # type: ignore\n",
    "            anchor_level=anchor_level\n",
    "        )\n",
    "\n",
    "    \n",
    "    if filter_expr is None:\n",
    "        filter_expr = _get_file_filters(curriculum)\n",
    "    \n",
    "    for file_fragment in dataset.get_fragments(): # type: ignore\n",
    "\n",
    "        table = file_fragment.to_table(\n",
    "            filter=filter_expr,\n",
    "            columns=['anchor_idx', 'positive_idx', 'negative_idx', 'anchor_distance']\n",
    "        )\n",
    "        \n",
    "        yield file_fragment, table\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# Triplet batch generator\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "def triplet_batches(\n",
    "    iter_files: Iterator[Tuple[ds.FileFragment, pa.Table]],\n",
    "    curriculum: CurriculumConfig,\n",
    "    rng: Optional[torch.Generator] = None,\n",
    ") -> Iterator[List[Tuple[int, int, int]]]:\n",
    "    \n",
    "    if rng is None:\n",
    "        rng = torch.Generator()\n",
    "        rng.manual_seed(curriculum.seed)\n",
    "\n",
    "    n_neg = curriculum.n_negatives\n",
    "    n_pos = curriculum.n_positives\n",
    "\n",
    "    for file_num, (file, file_batch) in enumerate(iter_files, start=1):\n",
    "\n",
    "        df_batch = (\n",
    "            pl\n",
    "            .from_arrow(file_batch)\n",
    "        )\n",
    "\n",
    "        if df_batch.height > n_pos:\n",
    "            df_batch = df_batch.sample(\n",
    "                n=curriculum.n_positives,\n",
    "                with_replacement=False,\n",
    "                shuffle=True,\n",
    "                seed=curriculum.seed\n",
    "            )\n",
    "\n",
    "        df = (\n",
    "            df_batch\n",
    "            .group_by('anchor_idx', 'positive_idx', maintain_order=True)  # type: ignore\n",
    "            .agg(\n",
    "                pl.col('negative_idx')\n",
    "            )\n",
    "            .group_by('anchor_idx', maintain_order=True)\n",
    "            .agg(\n",
    "                pl.col('positive_idx'),\n",
    "                pl.col('negative_idx')\n",
    "            )\n",
    "            .with_columns(\n",
    "                fallback=pl.col('negative_idx')\n",
    "                           .list.len()\n",
    "                           .lt(n_neg)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f'  Batch {file_num} '\n",
    "            f'[{Path(file.path).parent.stem}]: '\n",
    "            f'triplets = {df_batch.height:,}, '\n",
    "            f'grouped triplets = {df.height:,}'\n",
    "        )\n",
    "\n",
    "        complete = df.filter(pl.col('fallback')).drop('fallback')\n",
    "        incomplete = df.filter(~pl.col('fallback')).drop('fallback')\n",
    "\n",
    "        print(f'    Complete {complete.height:,}, Incomplete = {incomplete.height:,}')\n",
    "\n",
    "        if incomplete.height == 0:\n",
    "            completed = complete\n",
    "\n",
    "        elif complete.height == 0:\n",
    "            completed = _fill_incomplete(incomplete, curriculum.triplets_parquet, curriculum)\n",
    "\n",
    "        else:\n",
    "            _completed = _fill_incomplete(incomplete, curriculum.triplets_parquet, curriculum)\n",
    "\n",
    "            completed = (\n",
    "                pl\n",
    "                .concat([\n",
    "                    complete, \n",
    "                    _completed\n",
    "                ])\n",
    "            )\n",
    "\n",
    "        triplet_iter = (\n",
    "            completed\n",
    "            .explode('positive_idx', 'negative_idx')\n",
    "            .iter_rows(named=True)\n",
    "        )\n",
    "\n",
    "        triplets = []\n",
    "        for row in triplet_iter:\n",
    "            anchor = row['anchor_idx']\n",
    "            positive = row['positive_idx']\n",
    "            negatives = row['negative_idx']\n",
    "\n",
    "            triplets.append((anchor, positive))\n",
    "\n",
    "            if negatives:\n",
    "                for negative in negatives:\n",
    "                    triplets.append((anchor, negative))\n",
    "\n",
    "        yield triplets\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# Main logic\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "curriculum = CurriculumConfig(\n",
    "    anchor_level=[2, 3],\n",
    "    anchor_distance=[0.25, 0.5],\n",
    "    n_positives=100,\n",
    "    n_negatives=30\n",
    ")\n",
    "\n",
    "file_iterator = iter_file_batches(\n",
    "    dataset=None,\n",
    "    filter_expr=None,\n",
    "    codes_parquet=curriculum.codes_parquet,\n",
    "    triplets_parquet=curriculum.triplets_parquet,\n",
    "    curriculum=curriculum,\n",
    "    anchor_level=curriculum.anchor_level\n",
    " )\n",
    "\n",
    "triplets = triplet_batches(file_iterator, curriculum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8eb11514",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_dict = _get_distance_dict(curriculum.distance_parquet)\n",
    "relation_dict = _get_relation_dict(curriculum.relation_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f80b9262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_dict[(0, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c57c449d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 'child')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation_dict[(0, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3b205d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = [(0, i) for i in range(1, 51)]\n",
    "_, _, _, idx_to_code = get_indices_codes(curriculum.codes_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e410297",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets_ext = []\n",
    "for i, j in triplets:\n",
    "    dist = distance_dict.get((i, j), None)\n",
    "    rel_id, rel = relation_dict.get((i, j), (None, None))\n",
    "    triplets_ext.append(((i, j), (\n",
    "    (0, 1), (0, 2), (0, 3), (0, 4), (0, 5)) dist, rel_id, rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94decd06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1, 0.5, 0, 'child'),\n",
       " (0, 2, 1.5, 2, 'grandchild'),\n",
       " (0, 3, 2.5, 4, 'great-grandchild'),\n",
       " (0, 4, 3.5, 7, 'great-great-grandchild'),\n",
       " (0, 5, 2.5, 4, 'great-grandchild'),\n",
       " (0, 6, 3.5, 7, 'great-great-grandchild'),\n",
       " (0, 7, 2.5, 4, 'great-grandchild'),\n",
       " (0, 8, 3.5, 7, 'great-great-grandchild'),\n",
       " (0, 9, 2.5, 4, 'great-grandchild'),\n",
       " (0, 10, 3.5, 7, 'great-great-grandchild'),\n",
       " (0, 11, 2.5, 4, 'great-grandchild'),\n",
       " (0, 12, 3.5, 7, 'great-great-grandchild'),\n",
       " (0, 13, 2.5, 4, 'great-grandchild'),\n",
       " (0, 14, 3.5, 7, 'great-great-grandchild'),\n",
       " (0, 15, 2.5, 4, 'great-grandchild'),\n",
       " (0, 16, 3.5, 7, 'great-great-grandchild'),\n",
       " (0, 17, 3.5, 7, 'great-great-grandchild'),\n",
       " (0, 18, 1.5, 2, 'grandchild'),\n",
       " (0, 19, 2.5, 4, 'great-grandchild'),\n",
       " (0, 20, 3.5, 7, 'great-great-grandchild'),\n",
       " (0, 21, 3.5, 7, 'great-great-grandchild'),\n",
       " (0, 22, 1.5, 2, 'grandchild'),\n",
       " (0, 23, 2.5, 4, 'great-grandchild'),\n",
       " (0, 24, 3.5, 7, 'great-great-grandchild'),\n",
       " (0, 25, 2.5, 4, 'great-grandchild'),\n",
       " (0, 26, 3.5, 7, 'great-great-grandchild'),\n",
       " (0, 27, 2.5, 4, 'great-grandchild'),\n",
       " (0, 28, 3.5, 7, 'great-great-grandchild'),\n",
       " (0, 29, 3.5, 7, 'great-great-grandchild'),\n",
       " (0, 30, 3.5, 7, 'great-great-grandchild'),\n",
       " (0, 31, 3.5, 7, 'great-great-grandchild'),\n",
       " (0, 32, 3.5, 7, 'great-great-grandchild'),\n",
       " (0, 33, 3.5, 7, 'great-great-grandchild'),\n",
       " (0, 34, 3.5, 7, 'great-great-grandchild'),\n",
       " (0, 35, 1.5, 2, 'grandchild'),\n",
       " (0, 36, 2.5, 4, 'great-grandchild'),\n",
       " (0, 37, 3.5, 7, 'great-great-grandchild'),\n",
       " (0, 38, 3.5, 7, 'great-great-grandchild'),\n",
       " (0, 39, 2.5, 4, 'great-grandchild'),\n",
       " (0, 40, 3.5, 7, 'great-great-grandchild'),\n",
       " (0, 41, 3.5, 7, 'great-great-grandchild'),\n",
       " (0, 42, 1.5, 2, 'grandchild'),\n",
       " (0, 43, 2.5, 4, 'great-grandchild'),\n",
       " (0, 44, 3.5, 7, 'great-great-grandchild'),\n",
       " (0, 45, 2.5, 4, 'great-grandchild'),\n",
       " (0, 46, 3.5, 7, 'great-great-grandchild'),\n",
       " (0, 47, 2.5, 4, 'great-grandchild'),\n",
       " (0, 48, 3.5, 7, 'great-great-grandchild'),\n",
       " (0, 49, 2.5, 4, 'great-grandchild'),\n",
       " (0, 50, 3.5, 7, 'great-great-grandchild')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplets_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2349fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naics-gemini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
